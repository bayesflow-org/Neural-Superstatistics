{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import gamma, beta\n",
    "import talib\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../src\")\n",
    "from networks import DynamicGaussianNetworkJoint\n",
    "from priors import diffusion_prior, random_walk_prior\n",
    "from micro_models import dynamic_batch_diffusion, diffusion_trial, fast_dm_simulate\n",
    "from macro_models import random_walk_shared_var, random_walk\n",
    "from context import generate_design_matrix\n",
    "from transformations import scale_z, unscale_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu setting and checking\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_OBS      = 3200\n",
    "BATCH_SIZE = 8\n",
    "N_SAMPLES  = 2000\n",
    "N_PARAMS   = 6\n",
    "N_SIM      = 100\n",
    "\n",
    "PARAM_LABELS = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "PARAM_NAMES  = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']\n",
    "\n",
    "EMPIRIC_COLOR = '#1F1F1F'\n",
    "NEURAL_COLOR = '#852626'\n",
    "COMPARISON_COLOR = '#133a76'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MACRO_MEAN  = beta(a=1, b=25).mean()\n",
    "MACRO_STD   = beta(a=1, b=25).std()\n",
    "MICRO_MEANS = [1.75, 1.75, 1.75, 1.75, 1.7, 1] # calculated based on 10000 simulated theta_1:3200\n",
    "MICRO_STDS   = [1.5, 1.5, 1.5, 1.5, 1.25, 1] # calculated based on 10000 simulated theta_1:3200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams['font.sans-serif'] = \"Palatino\"\n",
    "matplotlib.rcParams['font.family'] = \"sans-serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y_true, y_pred):\n",
    "    return tf.reduce_mean(-y_pred.log_prob(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_fun(batch_size, T):\n",
    "    theta = diffusion_prior(batch_size, n_cond=N_PARAMS-2)\n",
    "    eta = random_walk_prior(batch_size, N_PARAMS)\n",
    "    theta_t = random_walk(theta, eta, T)\n",
    "    context = generate_design_matrix(batch_size, T)\n",
    "    rt = dynamic_batch_diffusion(theta_t, context).astype(np.float32)\n",
    "    x = tf.concat((rt, to_categorical(context[:, :, np.newaxis])), axis=-1)\n",
    "\n",
    "    eta_z = scale_z(eta, MACRO_MEAN, MACRO_STD)\n",
    "    \n",
    "    theta_t_z = theta_t.copy()\n",
    "    for i in range(theta_t.shape[0]):\n",
    "        theta_t_z[i] =  scale_z(theta_t[i], MICRO_MEANS,  MICRO_STDS)\n",
    "\n",
    "    return eta_z.astype(np.float32), theta_t_z.astype(np.float32), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eta_z, theta_t_z, x = generator_fun(BATCH_SIZE, N_OBS)\n",
    "theta_t_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_trainer(generator, network, optimizer, steps_per_epoch, p_bar):\n",
    "    losses = []\n",
    "    for step in range(1, steps_per_epoch+1):\n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "            # Simulate from model\n",
    "            macro_params, micro_params, data = generator() \n",
    "\n",
    "            # Forward pass\n",
    "            posterior = network(data)\n",
    "\n",
    "            # loss computation\n",
    "            T = int(micro_params.shape[1])\n",
    "            loss = nll(tf.concat([tf.stack([macro_params] * T, axis=1), micro_params], axis=-1), posterior)\n",
    "        \n",
    "        # One step backprop\n",
    "        g = tape.gradient(loss, network.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(g, network.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "        # Update progress bar\n",
    "        p_bar.set_postfix_str(\"Ep: {},Step {},Loss: {:.3f},Loss.Avg: {:.3f}\"\n",
    "                              .format(ep, step, loss.numpy(), np.mean(losses)))\n",
    "        p_bar.update(1)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3200\n",
    "batch_size = 8\n",
    "simulator = partial(generator_fun, T=T, batch_size=batch_size)\n",
    "epochs = 100\n",
    "steps_per_epoch = 1000\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=5000,\n",
    "    decay_rate=0.8,\n",
    "    staircase=True\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_settings = {\n",
    "    'embedding_lstm_units' : 512, \n",
    "    'embedding_gru_units': 512,\n",
    "    'embedding_dense_args': dict(units=256, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'posterior_dense_args': dict(units=128, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    'n_micro_params': 6,\n",
    "    'n_macro_params': 6\n",
    "}\n",
    "network = DynamicGaussianNetworkJoint(network_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses = []\n",
    "# for ep in range(1, epochs+1):\n",
    "#     with tqdm(total=steps_per_epoch, desc='Training epoch {}'.format(ep)) as p_bar:\n",
    "#         losses_ep = epoch_trainer(simulator, network, optimizer, steps_per_epoch, p_bar)\n",
    "#         losses.append(losses_ep)\n",
    "#     network.save_weights('../trained_networks/full_rw_ddm_joint')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.load_weights('../trained_networks/full_rw_ddm_joint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "data = pd.read_csv('../data/data_lexical_decision.csv', sep=',', header=0)\n",
    "\n",
    "# prepare data for fitting\n",
    "ids = np.unique(data.id)\n",
    "N_SUBS = len(ids)\n",
    "\n",
    "# negative rts for error responses\n",
    "data.rt.loc[data.acc == 0] = -data.rt.loc[data.acc == 0]\n",
    "\n",
    "# iterate over subjects\n",
    "x_nn = np.zeros((len(ids), N_OBS, 5))\n",
    "\n",
    "for id in ids:\n",
    "    person_data = data[data.id == id]\n",
    "    rt = np.array([person_data.rt])[:, :, np.newaxis]\n",
    "    stim_type = np.array([person_data.stim_type])[:, :, np.newaxis] - 1 \n",
    "    context = to_categorical(stim_type)\n",
    "    x_nn[id-1] = tf.concat((rt, context), axis=-1)\n",
    "\n",
    "x_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "post_eta_z = np.zeros((N_SAMPLES, N_SUBS, N_OBS, N_PARAMS))\n",
    "post_theta_t_z = np.zeros((N_SAMPLES, N_SUBS, N_OBS, N_PARAMS))\n",
    "for i in range(len(ids)):\n",
    "    post_eta_z[:, i:i+1, :, :], post_theta_t_z[:, i:i+1, :, :] = network.sample_n(x_nn[i:i+1], N_SAMPLES)\n",
    "    print(\"Sub nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta = unscale_z(post_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "post_theta_t = unscale_z(post_theta_t_z, MICRO_MEANS, MICRO_STDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read fast-dm parameter estimates\n",
    "fast_dm_params = pd.read_csv('../data/parameters_full_ddm_error_coding_cs.lst', encoding='iso-8859-1', header=0, delim_whitespace=True)\n",
    "fast_dm_params['dataset'] = fast_dm_params['dataset'].str.extract('(\\d+)').astype(int)\n",
    "fast_dm_params = fast_dm_params[['dataset', 'v_1', 'v_2', 'v_3', 'v_4', 'a', 't0', 'sv', 'st0']]\n",
    "fast_dm_params = fast_dm_params.sort_values('dataset')\n",
    "fast_dm_params = fast_dm_params.reset_index(drop=True)\n",
    "fast_dm_params = fast_dm_params.to_numpy()[:, 1:]\n",
    "fast_dm_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with fast_dm for all subjects\n",
    "pred_rt_fast_dm = np.zeros((N_SUBS, N_OBS))\n",
    "for i in range(N_SUBS):\n",
    "    context = data.stim_type.loc[data.id == i+1].to_numpy() - 1\n",
    "    pred_rt_fast_dm[i] = fast_dm_simulate(fast_dm_params[i], context)\n",
    "    \n",
    "pred_rt_fast_dm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: RT over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_check(emp_data, post_theta_t, n_sim, sma_period=5):\n",
    "    # get experimental context\n",
    "    context = emp_data.stim_type.values - 1\n",
    "    # get empirical response times\n",
    "    emp_rt = np.abs(emp_data.rt.values)\n",
    "    sma_emp_rt = talib.SMA(emp_rt, timeperiod=sma_period)\n",
    "    \n",
    "    # sample from posterior\n",
    "    idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/n_sim, dtype=np.int32)\n",
    "    theta = post_theta_t[idx]\n",
    "\n",
    "    n_obs = emp_rt.shape[0]\n",
    "    pred_rt = np.zeros((n_sim, n_obs))\n",
    "    sma_pred_rt = np.zeros((n_sim, n_obs))\n",
    "    # iterate over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "        # Iterate over number of trials\n",
    "        rt = np.zeros(n_obs)\n",
    "        for t in range(n_obs):\n",
    "            # Run diffusion process\n",
    "            rt[t] = diffusion_trial(theta[sim, t, context[t]], theta[sim, t, 4], theta[sim, t, 5])\n",
    "        pred_rt[sim] = np.abs(rt)\n",
    "        sma_pred_rt[sim] = talib.SMA(np.abs(rt), timeperiod=sma_period)\n",
    "\n",
    "    return pred_rt, sma_pred_rt, emp_rt, sma_emp_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with neural for all subjects\n",
    "pred_rt_neural = np.zeros((N_SUBS, N_SIM, N_OBS))\n",
    "sma_pred_rt_neural = np.zeros((N_SUBS, N_SIM, N_OBS))\n",
    "\n",
    "pred_rt_quantiles = np.zeros((N_SUBS, 2, N_OBS))\n",
    "pred_rt_medians = np.zeros((N_SUBS, N_OBS))\n",
    "\n",
    "emp_rt =  np.zeros((N_SUBS, N_OBS))\n",
    "sma_emp_rt =  np.zeros((N_SUBS, N_OBS))\n",
    "\n",
    "for sub in range(N_SUBS):\n",
    "    # predict RTs\n",
    "    person_data = data[data.id == sub+1]\n",
    "    pred_rt_neural[sub], sma_pred_rt_neural[sub], emp_rt[sub], sma_emp_rt[sub] = pr_check(person_data, post_theta_t[:, sub, :, :], N_SIM)\n",
    "    # compute RT quantiles\n",
    "    pred_rt_quantiles[sub] = np.quantile(sma_pred_rt_neural[sub], [0.025, 0.975], axis=0)\n",
    "    pred_rt_medians[sub] = np.median(sma_pred_rt_neural[sub], axis=0)\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=800\n",
    "emp_data_horizon = x_nn[:, :N_OBS-horizon, :]\n",
    "\n",
    "# inference on restircted data\n",
    "post_eta_z_horizon = np.zeros((N_SAMPLES, N_SUBS, N_OBS-horizon, N_PARAMS))\n",
    "post_theta_t_z_horizon = np.zeros((N_SAMPLES, N_SUBS, N_OBS-horizon, N_PARAMS))\n",
    "for i in range(len(ids)):\n",
    "    post_eta_z_horizon[:, i:i+1, :, :], post_theta_t_z_horizon[:, i:i+1, :, :] = network.sample_n(emp_data_horizon[i:i+1], N_SAMPLES)\n",
    "    print(\"Sub nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta_last = unscale_z(post_eta_z_horizon[:, :, -1, :], MACRO_MEAN, MACRO_STD)\n",
    "post_theta_last = unscale_z(post_theta_t_z_horizon[:, :, -1, :], MICRO_MEANS,  MICRO_STDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean posteriors\n",
    "post_eta_last_means = post_eta_last.mean(axis=0)\n",
    "post_theta_last_means = post_theta_last.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/N_SIM, dtype=np.int32)\n",
    "post_eta_last_select = post_eta_last[idx]\n",
    "post_theta_last_select = post_theta_last[idx]\n",
    "post_eta_last_select.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dynamic parameters and simulate RTs\n",
    "pred_rt_horizon = np.zeros((N_SIM, N_SUBS, horizon, 1))\n",
    "sma_pred_rt_horizon = np.zeros((N_SIM, N_SUBS, horizon, 1))\n",
    "\n",
    "context = x_nn[:, :, 1:].argmax(axis=2)[:, T-horizon:]\n",
    "\n",
    "for sub in range(N_SUBS):\n",
    "    for i in range(N_SIM):\n",
    "        pred_theta_t = random_walk(post_theta_last_select[i, sub:sub+1], post_eta_last_select[i, sub:sub+1], horizon)\n",
    "        pred_rt_horizon[i, sub:sub+1] = np.abs(dynamic_batch_diffusion(pred_theta_t, context[sub:sub+1]).astype(np.float32))\n",
    "        sma_pred_rt_horizon[i, sub, :, 0] = talib.SMA(pred_rt_horizon[i, sub, :, 0], timeperiod=5)\n",
    "\n",
    "    print(\"Sub nr. {} is predicted\".format(sub+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_horizon_medians = np.median(sma_pred_rt_horizon, axis=0)\n",
    "pred_rt_horizon_quantiles = np.quantile(sma_pred_rt_horizon, [0.025, 0.975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorderLegend2(ax=None,order=None,unique=False):\n",
    "    if ax is None: ax=plt.gca()\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels, handles = zip(*sorted(zip(labels, handles), key=lambda t: t[0])) # sort both labels and handles by labels\n",
    "    if order is not None: # Sort according to a given list (not necessarily complete)\n",
    "        keys=dict(zip(order,range(len(order))))\n",
    "        labels, handles = zip(*sorted(zip(labels, handles), key=lambda t,keys=keys: keys.get(t[0],np.inf)))\n",
    "    if unique:  labels, handles= zip(*unique_everseen(zip(labels,handles), key = labels)) # Keep only the first of each handle\n",
    "    ax.legend(handles, labels,\n",
    "              fontsize=16, loc='upper right')\n",
    "    return(handles, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "horizon = 800\n",
    "for sub in range(N_SUBS):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
    "                        gridspec_kw={'width_ratios': [6, 1]})\n",
    "    axrr = ax.flat\n",
    "    # plot empiric and predicted response times series\n",
    "    time = np.arange(N_OBS) \n",
    "\n",
    "    axrr[0].plot(time, sma_emp_rt[sub], color=EMPIRIC_COLOR, lw=1.4, alpha=0.8, label='SMA5: Empiric')\n",
    "    axrr[0].plot(time[:N_OBS-horizon], pred_rt_medians[sub, :N_OBS-horizon], color=NEURAL_COLOR, lw=1.4, label='SMA5: Post. re-simulation median', alpha=0.8)\n",
    "    axrr[0].plot(time[N_OBS-horizon:], pred_rt_horizon_medians[sub], color=\"#b35032\", lw=1.4, label='SMA5: Multi-horizon predictive median', alpha=0.6)\n",
    "    axrr[0].fill_between(time[N_OBS-horizon:], pred_rt_horizon_quantiles[0, sub, :, 0], pred_rt_horizon_quantiles[1, sub, :, 0], color=\"#b35032\", linewidth=0, alpha=0.4, label='Multi-horizon predictive 95% CI')\n",
    "    axrr[0].fill_between(time[:N_OBS-horizon], pred_rt_quantiles[sub, 0, :N_OBS-horizon], pred_rt_quantiles[sub, 1, :N_OBS-horizon], color=NEURAL_COLOR, linewidth=0, alpha=0.5, label='Post. re-simulation 95% CI')\n",
    "    \n",
    "    for idx in np.argwhere(person_data.session.diff().values == 1):\n",
    "        if idx == 800:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "        else:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.7)\n",
    "    for idx in np.argwhere(person_data.block.diff().values == 1):\n",
    "        if idx == 100:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "        else:\n",
    "            axrr[0].axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "    sns.despine(ax=axrr[0])\n",
    "    axrr[0].set_ylabel('RT(s)', fontsize=18, rotation=0, labelpad=40)\n",
    "    axrr[0].set_xlabel('\\nTrial', fontsize=18)\n",
    "    axrr[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    f.legend(fontsize=16, loc='center', \n",
    "            bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "\n",
    "    axrr[0].grid(False)\n",
    "    axrr[0].set_xticks([1, 800, 1600, 2400, 3200])\n",
    "\n",
    "    # plot empiric and predicted response time dist\n",
    "    plt.setp(ax, ylim=(0, 1.5))\n",
    "    sns.histplot(y=np.abs(emp_rt[sub]), fill=EMPIRIC_COLOR, color=EMPIRIC_COLOR, alpha=0.8, label=\"Empiric\", ax=axrr[1], stat=\"density\", bins=250, linewidth=0)#062759\n",
    "    sns.kdeplot(y=np.abs(pred_rt_fast_dm[sub]), fill= COMPARISON_COLOR, color=COMPARISON_COLOR, alpha=0.3, label=\"Fast-dm\", ax=axrr[1], linewidth=3.5)#598f70\n",
    "    sns.kdeplot(y=pred_rt_neural[sub].flatten(), fill=NEURAL_COLOR, color=NEURAL_COLOR, alpha=0.3, label=\"Dynamic DDM\", ax=axrr[1], linewidth=3.5)\n",
    "\n",
    "    axrr[1].legend(fontsize=16)\n",
    "    axrr[1].set_xlabel('', fontsize=18)\n",
    "    axrr[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "    axrr[1].set_yticklabels('')\n",
    "    axrr[1].set_xticklabels('')\n",
    "    axrr[1].xaxis.set_ticks([])\n",
    "    axrr[1].yaxis.set_ticks([])\n",
    "    axrr[1].get_xaxis().set_visible(False)\n",
    "    for line in axrr[1].get_lines():\n",
    "        line.set_alpha(1)\n",
    "    sns.despine(ax=axrr[1], bottom=True)\n",
    "\n",
    "    axrr[0].annotate('Re-simulation',\n",
    "                xy=(0.38, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].annotate('Prediction',\n",
    "                xy=(0.84, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].tick_params(length=8)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.05)\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/rt_time_series_sub_{}.pdf\".format(sub+1), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Parameter Dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors(dynamic_posterior, fast_dm_params, par_labels, par_names, \n",
    "                            ground_truths=None):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "        \n",
    "    means = dynamic_posterior.mean(axis=0)\n",
    "    stds = dynamic_posterior.std(axis=0)\n",
    "    \n",
    "    post_max = np.array(means).max(axis=0)\n",
    "    post_min = np.array(means).min(axis=0)\n",
    "    upper_y_ax = post_max + [1, 1, 1, 1, 0.2, 0.05]\n",
    "    lower_y_ax = post_min - [1, 1, 1, 1, 0.2, 0.05]\n",
    "\n",
    "    time = np.arange(x_nn.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        ci_upper = means[:, i] + stds[:, i]\n",
    "        ci_lower = means[:, i] - stds[:, i]\n",
    "        ax.plot(time, means[:, i], color=NEURAL_COLOR, label='Post. mean')\n",
    "        ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=0.6, linewidth=0, label='Post. std. deviation')\n",
    "\n",
    "        if ground_truths is not None:\n",
    "            ax.plot(time, ground_truths[:, i], color='black', linestyle='dashed', label='True Dynamic', lw=2)\n",
    "        sns.despine(ax=ax)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Trial', fontsize=18)\n",
    "            ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=20)\n",
    "        ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax.set_ylim(lower_y_ax[i], upper_y_ax[i])\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "        # vertical bars\n",
    "        for idx in np.arange(799, 2400, 800):\n",
    "            if idx == 799:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        for idx in np.arange(99, 3100, 100):\n",
    "            if idx == 99:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "            else:\n",
    "                ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "\n",
    "        # horizontal fast-dm params\n",
    "        if i <= 3:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "            ax.fill_between(time, fast_dm_params[i] - fast_dm_params[6], fast_dm_params[i] + fast_dm_params[6], color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Fast-dm inter-trial variability')\n",
    "        elif i == 4:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "        else:\n",
    "            ax.plot(time, np.repeat(fast_dm_params[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Fast-dm estimate', lw=2.5)\n",
    "            ax.fill_between(time, fast_dm_params[i] - fast_dm_params[7]/2, fast_dm_params[i] + fast_dm_params[7]/2, color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Fast-dm inter-trial variability')\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                     bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/param_dynamic_sub_{}.pdf\".format(sub+1), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in range(N_SUBS):\n",
    "    plot_dynamic_posteriors(post_theta_t[:, sub, :, :], fast_dm_params[sub], PARAM_LABELS, PARAM_NAMES)\n",
    "    print(\"Sub {} is finished\".format(sub+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Average Parameter Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute means and stds for neural and fast-dm parameters\n",
    "neural_means = post_theta_t.mean(axis=0).mean(axis=0)\n",
    "neural_stds = post_theta_t.mean(axis=0).std(axis=0)\n",
    "\n",
    "fast_dm_means = fast_dm_params.mean(axis=0)\n",
    "fast_dm_sd = fast_dm_params.std(axis=0)\n",
    "\n",
    "post_max = np.array(neural_means).max(axis=0).max()\n",
    "upper_y_ax = post_max + 1\n",
    "\n",
    "sigma_factors = [1]\n",
    "alphas = [0.6]\n",
    "\n",
    "time = np.arange(N_OBS)\n",
    "f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.plot(time, neural_means[:, i], color=NEURAL_COLOR, label='Average post. mean')\n",
    "    for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "        ci_upper = neural_means[:, i] + sigma_factor * neural_stds[:, i]\n",
    "        ci_lower = neural_means[:, i] - sigma_factor * neural_stds[:, i]\n",
    "        ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=alpha, linewidth=0, label='Std. deviation post. mean')\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.set_xlabel('Trial', fontsize=18)\n",
    "        ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "    ax.set_title(PARAM_LABELS[i] + ' ({})'.format(PARAM_NAMES[i]), fontsize=20)\n",
    "    ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "    ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "    ax.grid(False)\n",
    "\n",
    "    # vertical bars\n",
    "    for idx in np.arange(799, 2400, 800):\n",
    "        if idx == 799:\n",
    "            ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "        else:\n",
    "            ax.axvline(idx, color='black', linestyle='solid', lw=1.5, alpha=0.5)\n",
    "    for idx in np.arange(99, 3100, 100):\n",
    "        if idx == 99:\n",
    "            ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "        else:\n",
    "            ax.axvline(idx, color='black', linestyle='dotted', lw=1.5, alpha=0.4)\n",
    "\n",
    "    # horizontal fast-dm params\n",
    "    ax.plot(time, np.repeat(fast_dm_means[i], x_nn.shape[1]), color=COMPARISON_COLOR, alpha=1, label='Average Fast-dm estimate', lw=2.5)\n",
    "    ax.fill_between(time, fast_dm_means[i] - fast_dm_sd[i], fast_dm_means[i] + fast_dm_sd[i], color=COMPARISON_COLOR, alpha=0.3, linewidth=0, label='Std. deviation Fast-dm estimate')\n",
    "\n",
    "    f.subplots_adjust(hspace=0.5)\n",
    "    if i == 0:\n",
    "        f.legend(fontsize=16, loc='center', \n",
    "                    bbox_to_anchor=(0.5, -0.05),fancybox=False, shadow=False, ncol=4)\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Simulated data & Parameter Recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_check_simulation(emp_data, post_theta_t, n_sim, sma_period=5):\n",
    "    # get experimental context\n",
    "    context = emp_data[:, 1:].argmax(axis=1)\n",
    "    # get empirical response times\n",
    "    emp_rt = np.abs(emp_data[:, 0], dtype=np.float64)\n",
    "    sma_emp_rt = talib.SMA(emp_rt, timeperiod=sma_period)\n",
    "    \n",
    "    # sample from posterior\n",
    "    idx = np.arange(0, N_SAMPLES-1, N_SAMPLES/n_sim, dtype=np.int32)\n",
    "    theta = post_theta_t[idx]\n",
    "\n",
    "    n_obs = emp_rt.shape[0]\n",
    "    pred_rt = np.zeros((n_sim, n_obs))\n",
    "    sma_pred_rt = np.zeros((n_sim, n_obs))\n",
    "    # iterate over number of simulations\n",
    "    for sim in range(n_sim):\n",
    "        # Iterate over number of trials\n",
    "        rt = np.zeros(n_obs)\n",
    "        for t in range(n_obs):\n",
    "            # Run diffusion process\n",
    "            rt[t] = diffusion_trial(theta[sim, t, context[t]], theta[sim, t, 4], theta[sim, t, 5])\n",
    "        pred_rt[sim] = np.abs(rt)\n",
    "        sma_pred_rt[sim] = talib.SMA(np.abs(rt), timeperiod=sma_period)\n",
    "\n",
    "    return pred_rt, sma_pred_rt, emp_rt, sma_emp_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "true_eta_z, true_theta_t_z, true_data = generator_fun(BATCH_SIZE, N_OBS)\n",
    "true_theta_t_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amortized inference\n",
    "post_eta_z = np.zeros((N_SAMPLES, BATCH_SIZE, N_OBS, N_PARAMS))\n",
    "post_theta_t_z = np.zeros((N_SAMPLES, BATCH_SIZE, N_OBS, N_PARAMS))\n",
    "for i in range(BATCH_SIZE):\n",
    "    post_eta_z[:, i:i+1, :, :], post_theta_t_z[:, i:i+1, :, :] = network.sample_n(true_data[i:i+1], N_SAMPLES)\n",
    "    print(\"Sim nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta = unscale_z(post_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "post_theta_t = unscale_z(post_theta_t_z, MICRO_MEANS, MICRO_STDS)\n",
    "\n",
    "true_eta = unscale_z(true_eta_z, MACRO_MEAN, MACRO_STD)\n",
    "true_theta_t = unscale_z(true_theta_t_z, MICRO_MEANS, MICRO_STDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict data with neural for all subjects\n",
    "pred_rt_neural = np.zeros((BATCH_SIZE, N_SIM, N_OBS))\n",
    "sma_pred_rt_neural = np.zeros((BATCH_SIZE, N_SIM, N_OBS))\n",
    "\n",
    "pred_rt_quantiles = np.zeros((BATCH_SIZE, 2, N_OBS))\n",
    "pred_rt_medians = np.zeros((BATCH_SIZE, N_OBS))\n",
    "\n",
    "true_rt =  np.zeros((BATCH_SIZE, N_OBS))\n",
    "sma_true_rt =  np.zeros((BATCH_SIZE, N_OBS))\n",
    "\n",
    "for sim in range(BATCH_SIZE):\n",
    "    # predict RTs\n",
    "    single_data = true_data[sim].numpy()\n",
    "    pred_rt_neural[sim], sma_pred_rt_neural[sim], true_rt[sim], sma_true_rt[sim] = pr_check_simulation(single_data, post_theta_t[:, sim, :, :], N_SIM)\n",
    "    # compute RT quantiles\n",
    "    pred_rt_quantiles[sim] = np.quantile(sma_pred_rt_neural[sim], [0.025, 0.975], axis=0)\n",
    "    pred_rt_medians[sim] = np.median(sma_pred_rt_neural[sim], axis=0)\n",
    "    print(\"Sim nr. {} is predicted\".format(sim+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon=700\n",
    "true_data_horizon = true_data[:, :N_OBS-horizon, :]\n",
    "\n",
    "# inference on restircted data\n",
    "post_eta_z_horizon = np.zeros((N_SAMPLES, BATCH_SIZE, N_OBS-horizon, N_PARAMS))\n",
    "post_theta_t_z_horizon = np.zeros((N_SAMPLES, BATCH_SIZE, N_OBS-horizon, N_PARAMS))\n",
    "for i in range(BATCH_SIZE):\n",
    "    post_eta_z_horizon[:, i:i+1, :, :], post_theta_t_z_horizon[:, i:i+1, :, :] = network.sample_n(true_data_horizon[i:i+1], N_SAMPLES)\n",
    "    print(\"Sim nr. {} is fitted\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eta_last = unscale_z(post_eta_z_horizon[:, :, -1, :], MACRO_MEAN, MACRO_STD)\n",
    "post_theta_last = unscale_z(post_theta_t_z_horizon[:, :, -1, :], MICRO_MEANS, MICRO_STDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte mean posteriors\n",
    "post_eta_last_means = post_eta_last.mean(axis=0)\n",
    "post_theta_last_means = post_theta_last.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dynamic parameters and simulate RTs\n",
    "pred_rt_horizon = np.zeros((N_SIM, BATCH_SIZE, horizon, 1))\n",
    "sma_pred_rt_horizon = np.zeros((N_SIM, BATCH_SIZE, horizon, 1))\n",
    "\n",
    "context = true_data[:, :, 1:].numpy().argmax(axis=2)[:, T-horizon:]\n",
    "\n",
    "for sim in range(BATCH_SIZE):\n",
    "    for i in range(N_SIM):\n",
    "        pred_theta_t = random_walk(post_theta_last_means[sim:sim+1], post_eta_last_means[sim:sim+1], horizon)\n",
    "        pred_rt_horizon[i, sim:sim+1] = np.abs(dynamic_batch_diffusion(pred_theta_t, context[sim:sim+1]).astype(np.float32))\n",
    "        sma_pred_rt_horizon[i, sim, :, 0] = talib.SMA(pred_rt_horizon[i, sim, :, 0], timeperiod=5)\n",
    "\n",
    "    print(\"Sim nr. {} is predicted\".format(sim+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rt_horizon_medians = np.median(sma_pred_rt_horizon, axis=0)\n",
    "pred_rt_horizon_quantiles = np.quantile(sma_pred_rt_horizon, [0.025, 0.975], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize figure\n",
    "horizon = 700\n",
    "for sub in range(BATCH_SIZE):\n",
    "    f, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
    "                        gridspec_kw={'width_ratios': [6, 1]})\n",
    "    axrr = ax.flat\n",
    "    # plot empiric and predicted response times series\n",
    "    time = np.arange(N_OBS) \n",
    "\n",
    "    axrr[0].plot(time, sma_true_rt[sub], color=EMPIRIC_COLOR, lw=1.4, alpha=0.7, label='SMA5: Simulated')\n",
    "    axrr[0].plot(time[:N_OBS-horizon], pred_rt_medians[sub, :N_OBS-horizon], color=NEURAL_COLOR, lw=1.4, label='SMA5: Post. re-simulation median', alpha=0.8)\n",
    "    axrr[0].plot(time[N_OBS-horizon:], pred_rt_horizon_medians[sub], color=\"#b35032\", lw=1.4, label='SMA5: Multi-horizon predictive median', alpha=0.6)\n",
    "    axrr[0].fill_between(time[N_OBS-horizon:], pred_rt_horizon_quantiles[0, sub, :, 0], pred_rt_horizon_quantiles[1, sub, :, 0], color=\"#b35032\", linewidth=0, alpha=0.4, label='Multi-horizon predictive 95% CI')\n",
    "    axrr[0].fill_between(time[:N_OBS-horizon], pred_rt_quantiles[sub, 0, :N_OBS-horizon], pred_rt_quantiles[sub, 1, :N_OBS-horizon], color=NEURAL_COLOR, linewidth=0, alpha=0.5, label='Post. re-simulation 95% CI')\n",
    "    sns.despine(ax=axrr[0])\n",
    "    axrr[0].set_ylabel('RT(s)', fontsize=18, rotation=0, labelpad=40)\n",
    "    axrr[0].set_xlabel('\\nTrial', fontsize=18)\n",
    "    axrr[0].tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "    f.legend(fontsize=16, loc='center', \n",
    "            bbox_to_anchor=(0.5, -0.05), ncol=3)\n",
    "    \n",
    "    axrr[0].set_xticks([1, 800, 1600, 2400, 3200])\n",
    "\n",
    "    # plot empiric and predicted response time dist\n",
    "    plt.setp(ax, ylim=(0, np.nanmax(pred_rt_medians[sub])*1.5))\n",
    "    sns.histplot(y=np.abs(true_rt[sub]), fill=EMPIRIC_COLOR, color=EMPIRIC_COLOR, alpha=0.7, label=\"Simulated\", ax=axrr[1], stat=\"density\", bins=250, linewidth=0)\n",
    "    sns.kdeplot(y=pred_rt_neural[sub].flatten(), fill=NEURAL_COLOR, color=NEURAL_COLOR, alpha=0.3, label=\"Dynamic DDM\", ax=axrr[1], linewidth=3.5)\n",
    "\n",
    "    axrr[1].legend(fontsize=16)\n",
    "    axrr[1].set_xlabel('', fontsize=18)\n",
    "    axrr[1].tick_params(axis='both', which='major', labelsize=16)\n",
    "    axrr[1].set_yticklabels('')\n",
    "    axrr[1].set_xticklabels('')\n",
    "    axrr[1].xaxis.set_ticks([])\n",
    "    axrr[1].yaxis.set_ticks([])\n",
    "    axrr[1].get_xaxis().set_visible(False)\n",
    "    for line in axrr[1].get_lines():\n",
    "        line.set_alpha(1)\n",
    "    sns.despine(ax=axrr[1], bottom=True)\n",
    "\n",
    "    axrr[0].annotate('Re-simulation',\n",
    "                xy=(0.38, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].annotate('Prediction',\n",
    "                xy=(0.84, 1), xytext=(0, 20),\n",
    "                xycoords=('axes fraction', 'figure fraction'),\n",
    "                textcoords='offset points',\n",
    "                size=20, ha='center', va='top', weight=\"bold\")\n",
    "\n",
    "    axrr[0].tick_params(length=8)\n",
    "    axrr[0].grid(False)\n",
    "\n",
    "    plt.subplots_adjust(wspace = 0.05)\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/rt_time_series_simulation_{}.pdf\".format(sub+1), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dynamic_posteriors_simulation(dynamic_posterior, ground_truths, par_labels, par_names):\n",
    "    \"\"\"\n",
    "    Inspects the dynamic posterior given a single data set. Assumes six dynamic paramters.\n",
    "    \"\"\"\n",
    "        \n",
    "    means = dynamic_posterior.mean(axis=0)\n",
    "    std = dynamic_posterior.std(axis=0)\n",
    "    \n",
    "    post_max = np.array(means).max(axis=0)\n",
    "    post_min = np.array(means).min(axis=0)\n",
    "    upper_y_ax = post_max + [1, 1, 1, 1, 0.2, 0.05]\n",
    "    lower_y_ax = post_min - [1, 1, 1, 1, 0.2, 0.05]\n",
    "\n",
    "    sigma_factors = [1]\n",
    "    alphas = [0.5]\n",
    "\n",
    "    time = np.arange(dynamic_posterior.shape[1])\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        \n",
    "        ax.plot(time, means[:, i], color=NEURAL_COLOR, label='Post. mean', lw=1.5, alpha=0.8)\n",
    "        for sigma_factor, alpha in zip(sigma_factors, alphas):\n",
    "            ci_upper = means[:, i] + sigma_factor * std[:, i]\n",
    "            ci_lower = means[:, i] - sigma_factor * std[:, i]\n",
    "            ax.fill_between(time, ci_upper, ci_lower, color=NEURAL_COLOR, alpha=alpha, linewidth=0, label='Post. std. deviation')\n",
    "            ax.plot(time, ground_truths[:, i], color=EMPIRIC_COLOR, label='True Dynamic', lw=2, alpha=0.8)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Trial', fontsize=18)\n",
    "            ax.set_ylabel(\"Parameter value\", fontsize=18)\n",
    "\n",
    "\n",
    "        ax.set_title(par_labels[i] + ' ({})'.format(par_names[i]), fontsize=20)\n",
    "        ax.set_xticks([1, 800, 1600, 2400, 3200])\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        ax.set_ylim(lower_y_ax[i], upper_y_ax[i])\n",
    "\n",
    "        ax.grid(False)\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                     bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()\n",
    "    f.savefig(\"../plots/param_dynamic_simulation_{}.pdf\".format(sim+1), dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sim in range(BATCH_SIZE):\n",
    "    plot_dynamic_posteriors_simulation(post_theta_t[:, sim, :, :], true_theta_t[sim], PARAM_LABELS, PARAM_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAM_NAMES  = [r'$\\eta_{v_1}$', r'$\\eta_{v_2}$', r'$\\eta_{v_3}$', r'$\\eta_{v_4}$', r'$\\eta_{a}$', r'$\\eta_{\\tau}$']\n",
    "\n",
    "for sim in range(BATCH_SIZE):\n",
    "    f, axarr = plt.subplots(2, 3, figsize=(18, 8))\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        sns.kdeplot(post_eta[:, sim, 2999, i], ax=ax, color='#852626', fill='#852626', alpha=0.5, label=\"Post.\", lw=2)\n",
    "        ax.axvline(true_eta[sim, i], label='True', color=\"#062759\", lw=2)\n",
    "\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Parameter value', fontsize=18)\n",
    "            ax.set_ylabel(\"Density\", fontsize=18)\n",
    "        else:\n",
    "            ax.set_ylabel(\"\", fontsize=18)\n",
    "\n",
    "        ax.set_title('Eta ' + PARAM_LABELS[i] + ' ({})'.format(PARAM_NAMES[i]), fontsize=20)\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        ax.grid(False)\n",
    "\n",
    "\n",
    "        f.subplots_adjust(hspace=0.5)\n",
    "        if i == 0:\n",
    "            f.legend(fontsize=16, loc='center', \n",
    "                    bbox_to_anchor=(0.5, -0.05), ncol=4)\n",
    "\n",
    "    f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter recovery animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sim = 500\n",
    "n_post_samples = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# generate data\n",
    "eta_z, theta_t_z, data = generator_fun(n_sim, N_OBS)\n",
    "print(eta_z.shape)\n",
    "print(theta_t_z.shape)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chunks = 100\n",
    "chunk_len = int(data.shape[0] / n_chunks)\n",
    "chunk_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "counter = 0\n",
    "theta_z_pred = np.zeros((n_post_samples, n_sim, N_OBS, N_PARAMS), dtype=np.float32)\n",
    "eta_z_pred = np.zeros((n_post_samples, n_sim, N_OBS, N_PARAMS), dtype=np.float32)\n",
    "for x in tf.split(data, n_chunks, axis=0):\n",
    "    dists = network(x)\n",
    "    post = dists.sample(n_post_samples).numpy()\n",
    "    theta_z_pred[:, counter*chunk_len:(counter+1)*chunk_len] = post[:, :, :, 6:]\n",
    "    eta_z_pred[:, counter*chunk_len:(counter+1)*chunk_len] = post[:, :, :, :6]\n",
    "    print(counter)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_pred_means = np.mean(theta_z_pred, axis=0)\n",
    "theta_pred_stds = np.std(theta_z_pred, axis=0)\n",
    "eta_pred_mean = np.mean(eta_z_pred[:, :, -1, :], axis=0)\n",
    "eta_pred_std = np.std(eta_z_pred[:, :, -1, :], axis=0)\n",
    "theta_pred_means = unscale_z(theta_pred_means, MICRO_MEANS, MICRO_STDS)\n",
    "theta_pred_stds = theta_pred_stds * MICRO_STDS\n",
    "eta_pred_mean = unscale_z(eta_pred_mean, MACRO_MEAN, MACRO_STD)\n",
    "eta_pred_std = eta_pred_std * MACRO_STD\n",
    "theta_true = (theta_z_true * MICRO_STDS) + MICRO_MEANS\n",
    "eta_true = (eta_z_true * MACRO_MEAN) + MACRO_STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONT_SIZE_1 = 10\n",
    "FONT_SIZE_2 = 9\n",
    "FONT_SIZE_3 = 7\n",
    "FONT_SIZE_4 = 3\n",
    "\n",
    "PARAM_LABELS = ['Drift rate 1', 'Drift rate 2', 'Drift rate 3', 'Drift rate 4', 'Threshold', 'Non-decision time']\n",
    "PARAM_NAMES  = [r'$v_1$', r'$v_2$', r'$v_3$', r'$v_4$', r'$a$', r'$\\tau$']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 6, figsize=(12, 3))\n",
    "from celluloid import Camera\n",
    "camera = Camera(f)\n",
    "for t in np.arange(0, 3201, step=13):\n",
    "    for i, ax in enumerate(axarr.flat):\n",
    "        ax.grid(alpha=0.5)\n",
    "        # content\n",
    "        ax.scatter(theta_true[:, t, i], theta_pred_means[:, t, i], alpha=0.5, color=NEURAL_COLOR)\n",
    " \n",
    "        # Make plots quadratic to avoid visual illusions\n",
    "        lower = min(theta_true[:, :, i].min(), theta_pred_means[:, :, i].min())\n",
    "        upper = max(theta_true[:, :, i].max(), theta_pred_means[:, :, i].max())\n",
    "        eps = (upper - lower) * 0.1\n",
    "        ax.set_xlim([lower - eps, upper + eps])\n",
    "        ax.set_ylim([lower - eps, upper + eps])\n",
    "\n",
    "        ax.plot([ax.get_xlim()[0], ax.get_xlim()[1]], [ax.get_ylim()[0], ax.get_ylim()[1]], \n",
    "                 color='black', alpha=0.9, linestyle='dashed')\n",
    "\n",
    "\n",
    "        # description\n",
    "        ax.set_title(PARAM_LABELS[i] + ' ({})'.format(PARAM_NAMES[i]), fontsize=FONT_SIZE_1)\n",
    "        if i == 0:\n",
    "            ax.set_xlabel('Ground truth', fontsize=FONT_SIZE_2)\n",
    "            ax.set_ylabel('Estimated', fontsize=FONT_SIZE_2)\n",
    "        if i == 2:\n",
    "            ax.annotate(r'$t =${}'.format(t), xy=(1, -0.4), xycoords=('axes fraction', 'axes fraction'),\n",
    "                        xytext=(0, 0), textcoords='offset points', size=FONT_SIZE_1, ha='center', va='bottom')\n",
    "        r2 = r2_score(theta_true[:, t, i], theta_pred_means[:, t, i])\n",
    "        ax.text(0.1, 0.8, '$R^2$ = {:.3f}'.format(r2),\n",
    "                    horizontalalignment='left',\n",
    "                    verticalalignment='center',\n",
    "                    transform=ax.transAxes, \n",
    "                    size=FONT_SIZE_3)\n",
    "        corr = np.corrcoef(theta_true[:, t, i], theta_pred_means[:, t, i])[0, 1]\n",
    "        ax.text(0.1, 0.7, '$r$ = {:.3f}'.format(corr),\n",
    "                        horizontalalignment='left',\n",
    "                        verticalalignment='center',\n",
    "                        transform=ax.transAxes, \n",
    "                        size=FONT_SIZE_3)\n",
    "\n",
    "        # aesthetics\n",
    "        ax.axis('square')\n",
    "        \n",
    "        sns.despine(ax=ax)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=FONT_SIZE_4)\n",
    "\n",
    "    plt.pause(0.00001)\n",
    "    camera.snap()\n",
    "\n",
    "animation = camera.animate()\n",
    "animation.save('../plots/param_recovery_animation.gif', dpi=100, writer='PillowWriter', fps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c85bf36f462aee8672315966a66dd5e91fa71003ac562e7969aa481cd7b291c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
